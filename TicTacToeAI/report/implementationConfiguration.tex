\section{Configuration of Implementation Platform}

Once the basic framework for our implementation was decided upon, the next step was to configure it to fit our needs. This meant defining the structure of the neural net we wanted to use as well as the rewards/ punishments the algorithm would receive for each possible action.

We initially based our decisions regarding the neural net's structure on other examples of game AIs using a similar approach, such as the learning RC-car (\cite{Harvey2017}) project as well as the "Keras plays Catch" example (\cite{Santana17}). However while we were able to run our code with these structures and some slight modifications, neither of them yielded the results we wanted to see: The AI did not seem to actually learn the game.

Afterwards we decided to start from scratch and implement the structure from the ground up: A single layer of densely connected neurons was the result of this process: It takes 9 input arguments (the values of each of the segments of the Tic Tac Toe field, e.g. "empty", "own mark" or "enemy mark" expressed as integers) and produces 9 outputs (a confidence value between 0 and 1 as a float depicting the probability that the corresponding action would be rewarded highly). By selecting the action associated with the highest predicted probability of success, the AI then "decides" upon an action to take.
\begin{lstlisting}[frame=single,language=Python,caption={Implementation of our neural network},captionpos=b]
self.model = Sequential()

# input size of 9 (rewards for each possible move)
# output size of 9 (q values for each possible move)
self.model.add(Dense(9, input_dim=9,
                     kernel_initializer='normal',
                     activation='relu'))
self.model.compile(loss='mse', optimizer=RMSprop())
\end{lstlisting}

\begin{lstlisting}[frame=single,language=Python,caption={Method of },captionpos=b]
def play(self, game_state, callback, first_move=False):             
    self.game_state = game_state  # save old game state
    if random.random() < self.epsilon:
       # select random field segment to play on
       self.action = random.randint(0, 8)
    else:
       # target to predict from is the current 
       #game state
       target = game_state.tolist()

       qvals = self.model.model.predict(np.array([target]),
       					 batch_size=1)

       # get the action with the highest value
       self.action = np.argmax(qvals)
    callback(self.action, self.mark, self.reward_me)
\end{lstlisting}

The next step was to define the rewards and punishments for each of the actions that the AI could possibly carry out: We started this process by defining what possible game moves were. Later, we decided how big of a reward each action should yield:

\begin{table}[H]
	\begin{tabular}{|l|c|}\hline
		Action & Reward \\ \hline \hline
		Placing a mark on any empty square & 100 \\ \hline
		Placing a mark to block the enemy from placing two marks in a row & 200 \\ \hline
		Placing a mark to have two marks next to each other & 300 \\ \hline
		Placing a mark to block an enemy win & 400 \\ \hline
		Winning the game & 500 \\ \hline
		Losing the game & -500 \\ \hline
		Game ends in a draw & 0 \\ \hline
		Making an invalid move & -2000 \\ \hline
	\end{tabular}
	\caption{Table with rewards}
	\label{tablRewards}
\end{table}


\begin{lstlisting}[frame=single,language=Python,caption={Method of },captionpos=b]
def reward_me(self, rewards, new_game_state, game_over=False):
    X = np.array([new_game_state])
    Y = np.array([rewards.tolist()])
    self.model.model.fit(X, Y, verbose=False)
\end{lstlisting}

Implementing the overall game framework therefore also involved checking the AI's actions to see whether any of these moves were done.